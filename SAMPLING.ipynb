{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKTXzmkbdI3J9+RUC3JumE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAMARTH-1301/SAMPLING_SAMARTH_102203487/blob/main/SAMPLING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPsHx4Abh-Ce",
        "outputId": "7486cb7a-bd00-43c1-8972-adfbb1103b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Sampling1  Sampling2  Sampling3  Sampling4  Sampling5 Best Sampling Method\n",
            "Logistic Regression   0.921397   0.899563   0.895197   0.917391   0.903930            Sampling1\n",
            "Random Forest         0.995633   1.000000   0.995633   1.000000   1.000000            Sampling2\n",
            "SVM                   0.681223   0.707424   0.742358   0.756522   0.676856            Sampling4\n",
            "KNN                   0.947598   0.965066   0.943231   0.965217   0.943231            Sampling4\n",
            "Neural Network        0.969432   0.982533   0.991266   0.991304   1.000000            Sampling5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/Creditcard_data.csv')\n",
        "\n",
        "majority_class = data[data.Class == 0]\n",
        "minority_class = data[data.Class == 1]\n",
        "\n",
        "minority_upsampled = resample(minority_class,\n",
        "                              replace=True,\n",
        "                              n_samples=len(majority_class),\n",
        "                              random_state=42)\n",
        "\n",
        "balanced_data = pd.concat([majority_class, minority_upsampled])\n",
        "\n",
        "sample_size = int(0.5 * len(balanced_data))\n",
        "\n",
        "sample1 = balanced_data.sample(n=sample_size, random_state=1)\n",
        "\n",
        "_, sample2 = train_test_split(balanced_data, test_size=sample_size, stratify=balanced_data['Class'], random_state=2)\n",
        "\n",
        "step = len(balanced_data) // sample_size\n",
        "sample3_indices = np.arange(0, len(balanced_data), step)[:sample_size]\n",
        "sample3 = balanced_data.iloc[sample3_indices]\n",
        "\n",
        "clusters = np.array_split(balanced_data.sample(frac=1, random_state=4), 10)\n",
        "sample4 = pd.concat(clusters[:len(clusters) // 2])\n",
        "\n",
        "oversample = resample(minority_upsampled, replace=True, n_samples=sample_size // 2, random_state=5)\n",
        "sample5 = pd.concat([oversample, majority_class.sample(n=sample_size // 2, random_state=5)])\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=0),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
        "    \"SVM\": SVC(random_state=0),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Neural Network\": MLPClassifier(max_iter=1000, random_state=0)\n",
        "}\n",
        "\n",
        "model_accuracies = {model_name: [] for model_name in models.keys()}\n",
        "\n",
        "samples = [sample1, sample2, sample3, sample4, sample5]\n",
        "for model_name, model in models.items():\n",
        "    for sample in samples:\n",
        "        X = sample.drop(columns=['Class', 'Time'])\n",
        "        y = sample['Class']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        model_accuracies[model_name].append(accuracy)\n",
        "\n",
        "best_sampling_methods = {}\n",
        "for model_name, accuracies in model_accuracies.items():\n",
        "    best_sampling_index = np.argmax(accuracies)\n",
        "    best_sampling_methods[model_name] = f\"Sampling{best_sampling_index + 1}\"\n",
        "\n",
        "results = pd.DataFrame.from_dict(model_accuracies, orient='index')\n",
        "results.columns = [f\"Sampling{i+1}\" for i in range(len(samples))]\n",
        "results[\"Best Sampling Method\"] = list(best_sampling_methods.values())\n",
        "\n",
        "print(results.to_string())\n",
        "\n",
        "\n"
      ]
    }
  ]
}